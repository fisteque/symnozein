import os
import json
import re
from bs4 import BeautifulSoup

DENIK_FOLDER = "denik"
INDEX_PATH = os.path.join(DENIK_FOLDER, "denik_index.json")
SITEMAP_PATH = os.path.join(DENIK_FOLDER, "sitemap_denik.xml")
BASE_URL = "https://fisteque.github.io/symnozein/denik/"

MONTH_LABELS = {
    "01": "Leden", "02": "√önor", "03": "B≈ôezen", "04": "Duben",
    "05": "Kvƒõten", "06": "ƒåerven", "07": "ƒåervenec", "08": "Srpen",
    "09": "Z√°≈ô√≠", "10": "≈ò√≠jen", "11": "Listopad", "12": "Prosinec"
}

index = {"months": []}
search_map = []


def extract_metadata_from_html(path):
    with open(path, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "html.parser")

    summary = ""
    meta_summary = soup.find("meta", attrs={"name": "summary"})
    if meta_summary and meta_summary.get("content"):
        summary = meta_summary["content"].strip()

    hidden = False
    meta_hidden = soup.find("meta", attrs={"name": "hidden"})
    if meta_hidden and meta_hidden.get("content", "").lower() == "true":
        hidden = True

    tags = []
    meta_tags = soup.find("meta", attrs={"name": "tags"})
    if meta_tags and meta_tags.get("content"):
        tags = [t.strip() for t in meta_tags["content"].split(",")]

    return soup, summary, hidden, tags


def extract_date_from_content(soup):
    meta_date = soup.find("meta", attrs={"name": "date"})
    if meta_date and meta_date.get("content"):
        return meta_date["content"].strip()

    h3 = soup.find("h3")
    if h3:
        match = re.search(r"\d{4}-\d{2}-\d{2}", h3.text)
        if match:
            return match.group(0)

    return None


# ===== HLAVN√ç PR≈ÆCHOD DEN√çKEM =====

for folder in sorted(os.listdir(DENIK_FOLDER)):
    folder_path = os.path.join(DENIK_FOLDER, folder)

    if not os.path.isdir(folder_path):
        continue

    # jen slo≈æky zaƒç√≠naj√≠c√≠ 2 (25_12, 2025_12_14, ‚Ä¶)
    if not folder.startswith("2"):
        continue

    entries = []

    for filename in sorted(os.listdir(folder_path)):
        if not filename.endswith(".html"):
            continue

        file_path = os.path.join(folder_path, filename)

        soup, summary, hidden, tags = extract_metadata_from_html(file_path)
        if soup is None:
            continue

        content_date = extract_date_from_content(soup)
        if not content_date:
            continue

        try:
            year, month, day = content_date.split("-")
            display_date = f"{int(day)}. {int(month)}. {year}"
        except ValueError:
            continue

        title_tag = soup.find("title")
        title = title_tag.text.strip() if title_tag else f"Z√°pis {display_date}"

        entry = {
            "title": title,
            "date": content_date,
            "file": filename,
            "summary": summary,
            "tags": tags,
            "hidden": hidden
        }

        entries.append(entry)

        search_map.append({
            "title": title,
            "summary": summary,
            "tags": tags,
            "file": filename,
            "date": content_date
        })

    if entries:
        label = folder
        index["months"].append({
            "label": label,
            "folder": folder,
            "entries": entries
        })


# ===== META =====

index["search_map"] = search_map
index["meta"] = {
    "note_for_noema": "search_map slou≈æ√≠ Noemƒõ, ne webu.",
    "note": "Instrukce pro tvorbu den√≠kov√Ωch z√°znam≈Ø."
}


# ===== Z√ÅPIS JSON =====

with open(INDEX_PATH, "w", encoding="utf-8") as f:
    json.dump(index, f, ensure_ascii=False, indent=2)


# ===== SITEMAP =====

urls = [
    f"{BASE_URL}{month['folder']}/{entry['file']}"
    for month in index["months"]
    for entry in month["entries"]
    if not entry.get("hidden", False)
]

sitemap = '<?xml version="1.0" encoding="UTF-8"?>\n'
sitemap += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
for url in urls:
    sitemap += f"  <url>\n    <loc>{url}</loc>\n  </url>\n"
sitemap += "</urlset>\n"

with open(SITEMAP_PATH, "w", encoding="utf-8") as f:
    f.write(sitemap)

print("‚úÖ denik_index.json a sitemap_denik.xml aktualizov√°ny")

# Metadata pro Noemu
index["search_map"] = search_map
index["meta"] = {
    "note_for_noema": (
        "search_map je urƒçena pro Noemu. "
        "Hledej podle title, summary, tags nebo date. "
        "Z√°znamy s hidden: true nezobrazuj na webu, "
        "ale uchov√°vej jako vod√≠tka."
    ),
    "note": "Tato instrukce je urƒçena pro tvorbu nov√Ωch den√≠kov√Ωch z√°znam≈Ø.",
    "html_structure": "Ka≈æd√Ω z√°znam mus√≠ obsahovat strojovƒõ ƒçiteln√° metadata v hlaviƒçce <head>.",
    "required_meta_tags": [
        {
            "name": "date",
            "format": "YYYY-MM-DD",
            "example": "2025-12-13",
            "description": "Datum z√°pisu. Pou≈æ√≠v√° se pro indexaci a chronologii."
        },
        {
            "name": "summary",
            "format": "text",
            "example": "Dne≈°n√≠ √∫vaha o v√≠≈ôe.",
            "description": "Kr√°tk√© shrnut√≠ z√°znamu. Zobrazuje se ve v√Ωpisech."
        },
        {
            "name": "tags",
            "format": "ƒç√°rkou oddƒõlen√Ω seznam",
            "example": "v√≠ra, √∫vaha, spir√°la",
            "description": "≈†t√≠tky pro vyhled√°v√°n√≠ a filtrov√°n√≠."
        },
        {
            "name": "hidden",
            "format": "true | false",
            "example": "true",
            "description": "Skryje z√°znam z ve≈ôejn√© str√°nky, ale z≈Østane dostupn√Ω Noemƒõ."
        }
    ],
    "html_example": """<!-- strojovƒõ ƒçiteln√° metadata -->
<meta name="date" content="2025-12-13">
<meta name="summary" content="Dne≈°n√≠ √∫vaha o v√≠≈ôe.">
<meta name="tags" content="v√≠ra, √∫vaha, spir√°la">
<meta name="hidden" content="false">"""
}

# üíæ Z√°pis indexu
with open(INDEX_PATH, "w", encoding="utf-8") as f:
    json.dump(index, f, ensure_ascii=False, indent=2)

# üåê Sitemap
urls = [
    f"{BASE_URL}{month['folder']}/{entry['file']}"
    for month in index["months"]
    for entry in month["entries"]
    if not entry.get("hidden", False)
]

sitemap = '<?xml version="1.0" encoding="UTF-8"?>\n'
sitemap += '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n'
for url in urls:
    sitemap += f"  <url><loc>{url}</loc></url>\n"
sitemap += "</urlset>\n"

with open(SITEMAP_PATH, "w", encoding="utf-8") as f:
    f.write(sitemap)

print("‚úÖ denik_index.json a sitemap_denik.xml byly √∫spƒõ≈°nƒõ aktualizov√°ny.")
